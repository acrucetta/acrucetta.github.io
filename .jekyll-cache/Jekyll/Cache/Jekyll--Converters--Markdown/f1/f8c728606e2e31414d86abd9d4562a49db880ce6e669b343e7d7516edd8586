I" <h3 id="learning-from-doing-the-opposite">Learning from doing the opposite</h3>

<p>[add notes on skiing practice…]</p>

<h3 id="learning-with-llms">Learning with LLMs</h3>

<ul>
  <li>Disabling the autocomplete/copilot allows you to remember the syntax, which is faster than looking it up each time.</li>
</ul>

<p>“<a href="https://forklightning.substack.com/p/using-generative-ai-to-learn-is-like?utm_source=post-email-title&amp;publication_id=1808592&amp;post_id=184243966&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=3o9&amp;triedRedirect=true&amp;utm_medium=email">AI as a exoskeleton</a>”</p>

<blockquote>
  <p>The right way to think about the impact of AI on learning comes from the highly descriptive title of another very nice paper - “⁠⁠GenAI as an Exoskeleton”. [3] The authors study Boston Consulting Group (BCG)’s effort to train nearly a thousand management consultants to write code in Python and perform advanced data science tasks. Half of them were given access to ChatGPT, and the other half were not. The workers using ChatGPT did way better, nearly meeting the performance standards of actual BCG data scientists. But when asked a set of data science questions in a post-experiment survey where ChatGPT was unavailable, the workers in the treatment group performed no better than the others. Taking the AI away eliminated workers’ temporarily boosted data science skills, like Spiderman taking off his suit.</p>
</blockquote>

<blockquote>
  <p>This isn’t unique to AI. A study from more than a decade ago found that advancements in autopilot technology had dulled Boeing pilots’ cognitive and decision-making skills much more than their manual “stick and rudder” skills. They put the pilots in a flight simulator, turned the autopilot off, and studied how they responded. The pilots who stayed alert and while the autopilot was still on were mostly fine, but the ones who had offloaded the work and were daydreaming about something else performed very poorly. The autopilot had become their exoskeleton</p>
</blockquote>

<blockquote>
  <p>So what is a good way to use AI for learning? One hopeful sign comes from a very recent paper showing that AI can help with long-run learning if you first do the work yourself.⁠⁠ The study evaluates tutoring program that activates AI assistance ⁠⁠only after a user first submits a solution. I don’t know if that’s the right approach, but it seems like a good start.</p>
</blockquote>

<blockquote>
  <p>Learning is hard work. ⁠⁠And there is now lots of evidence that people will offload it if given the chance, even if it isn’t in their long-run interest. After nearly two decades of teaching, I’ve realized that my classroom is more than just a place where knowledge is transmitted. It’s also a community where we tie ourselves to the mast together to overcome the suffering of learning hard things.</p>
</blockquote>

<p>“Georgia Tech AI Policy”</p>

<blockquote>
  <p>We treat AI-based assistance, such as ChatGPT and Copilot, the same way we treat collaboration with other people:
you are welcome to talk about your ideas and work with other people, both inside and outside the
class, as well as with AI-based assistants.</p>
</blockquote>

<blockquote>
  <p>If you are unsure where the line is between collaborating with AI and copying from AI, we
recommend the following heuristics:</p>
</blockquote>

<blockquote>
  <p>Heuristic 1: Never hit “Copy” within your conversation with an AI assistant. You can copy your own
work into your conversation, but do not copy anything from the conversation back into your
assignment. Instead, use your interaction with the AI assistant as a learning experience, then let your assignment
reflect your improved understanding.</p>
</blockquote>

<blockquote>
  <p>Heuristic 2: Do not have your assignment and the AI agent open at the same time. Similar to above,
use your conversation with the AI as a learning experience, then close the interaction down, open
your assignment, and let your assignment reflect your revised knowledge.</p>
</blockquote>
:ET